\documentclass{article}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
\begin{document}

\title{Command and Control Subsystems Report}
\author{Jake Vossen: OREPACKAGERS}

\maketitle

% \begin{abstract}
% The abstract text goes here.
% \end{abstract}

\section{Subsystem Description}
The Command and Control subsystem is the subsystem responsible for
converting the requests that have been collected into downloaded data
to be distributed to users. It starts by receiving a list of
\texttt{request} objects - a structure for contating information about
each request. To prevent confusion, the mono-spaced \texttt{request}
will refer to they Python object itself, wheras plain ``request''
refers to the concept of a user request.

With this list of requests, the first thing it does is use Pythons
\texttt{multiprocessing}[1] library to split work up between the
different threads on the computer. While this software is designed
for low end machines to be more accessable to developing areas, most
computers[2] in recent times will have more than 1 CPU core (including
the Raspberry Pi[3]). This allows for the processor to split up all
the requests, and execute them in paralel, instead of waiting for each
one to finish individually, which can provide a large preformance
boost.

When downloading a \texttt{request}, it determens the type of
request. The types are URL, search, youtube, and ipfs. The steps for
each type of request is outlined below. 

\subsection{URLs}

URLs are your basic websites, such as
\url{https://en.wikipedia.org/wiki/Monty_Python_and_the_Holy_Grail},
or
\url{https://www.nytimes.com/2019/03/27/technology/turing-award-ai.html}. This
is for users who already know the content they want. In the backend,
the Python program is going to use the \texttt{wget}[4]
utility. Specifically, \texttt{wget -E -H -k -K -p -P path url
  robots=off} where \texttt{path} is the output directory and
\texttt{url} is the url that has been requested. To break it down:
\begin{itemize}
  \item \texttt{-E} tells \texttt{wget} to change the file extention
    if the url isn't a .html file. This allows for the downloading of
    PDF files as well as HTML files
  \item \texttt{-H} Tells \texttt{wget} that it is okay to download
    material from hosts that aren't from the specified URL. While this
    seems backwards at first, many websites host their fonts or
    pictures in a place that isn't the same as the document that is
    being request. This allows the page to appear just as it would
    when visited in a web browser
  \item \texttt{-k} This stands for ``convert links'', which means
    that when the download is complete, it converts the links on the
    page so they are sudible for browsing on the local machine. For
    example, if a blog has \texttt{otherwebsite.com/picture} on it, it
    will replace that with just \texttt{picture} to ensure that the
    browser will use the local versions of that picture
  \item \texttt{-K} This means that \texttt{wget} will make a backup
    of the HTML file when converting links with the \texttt{-k}
    option.
  \item \texttt{-p} is the most important option, as it tells
    \texttt{wget} to download all the requirements as well as the
    url. So if the site links to an outside source (such as
    \texttt{otherwebsite.com/picture}) also gets downloaded if it is
    linked in the requested url.
  \end{itemize}
All of those options ensures that downloading the URL requested get's
the website exactly as it appears in a browser, including linked
images. Additionally, it works with PDF and ZIP files, which is really
important to ensure all possible media can be obtained. This method is
also used by other parts of the program.

\subsection{Search}

Sometimes the user will not know exactly what they want, so we added
an option to get the first page of Google results (top 10
results). The \texttt{googlesearch}[5] library was very helpful for
this. This library provides a list of URLs, and then we use the URL
method to download those results (or the youtube download option if it
is a youtube link). The results are each in thier own folder nammed
based on the google search rank (1 is first result, 2 is second
result, etc). 

\subsection{YouTube}

It is well known that lot of quality educational and entiertainment
content is in video format, and the majority of that content is on
YouTube. That is why we are adding functionallity to request YouTube
videos (through a link, or a result from the search function). In this
case, the \texttt{youtube-dl} program allows for content retrevial.

\subsection{IPFS}
IPFS stands for ``InterPlanetary File System'', which is a ``A
peer-to-peer hypermedia protocol to make the web faster, safer, and
more open''[7]. The internet that is familiar to most people is the
client-server model[8], but IPFS changes that so everyone is both a
client and a server. Media is distribtued based of their crytographic
hash, a unique ID for each object instead of a URL. Anybody can add
objects, and when requesting an object, it can be downloaded from any
number of servers, not just the origional person hosting the
server. The \texttt{ipfs} command line utility[9] is used to retreive
objects.


\end{document}

%[1] https://docs.python.org/3.7/library/multiprocessing.html
%[2] https://www.pcbenchmarks.net/number-of-cpu-cores.html
%[3] https://www.raspberrypi.org/products/raspberry-pi-3-model-b/
%[4] https://www.gnu.org/software/wget/manual/wget.html
%[5] https://github.com/MarioVilas/googlesearch
%[6] https://github.com/ytdl-org/youtube-dl/
%[7] https://ipfs.io/
%[8] https://web.cs.wpi.edu/~cs513/s07/week1-unixsock.pdf
%[9] https://docs.ipfs.io/introduction/install/