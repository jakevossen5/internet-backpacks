\documentclass{article}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
\usepackage{graphicx}
\begin{document}

\title{Command and Control Subsystems Report}
\author{Jake Vossen: OREPACKAGERS}

\maketitle

% \begin{abstract}
% The abstract text goes here.
% \end{abstract}

\section{Subsystem Description}
The Command and Control subsystem is the subsystem responsible for
converting the requests that have been collected into downloaded data
to be distributed to users. It starts by receiving a list of
\texttt{request} objects - a structure for contating information about
each request. To prevent confusion, the mono-spaced \texttt{request}
will refer to they Python object itself, wheras plain ``request''
refers to the concept of a user request.

With this list of requests, the first thing it does is use Pythons
\texttt{multiprocessing}[1] library to split work up between the
different threads on the computer. While this software is designed
for low end machines to be more accessable to developing areas, most
computers[2] in recent times will have more than 1 CPU core (including
the Raspberry Pi[3]). This allows for the processor to split up all
the requests, and execute them in paralel, instead of waiting for each
one to finish individually, which can provide a large preformance
boost.

When downloading a \texttt{request}, it determens the type of
request. The types are URL, search, youtube, and ipfs. It follows this
flow chart to decide what to do:

\begin{figure}[h]
  \centering
  \includegraphics[scale=.6]{log-flow-chart.png}
  \caption{\begingroup \fontsize{10pt}{12pt}\selectfont Diagram describing flow chart of downloading each
    \texttt{request}. Jake Vossen, 2019-04-01 \endgroup}
\end{figure}

The steps for each type of request is outlined below.

\subsection{URLs}

URLs are your basic websites, such as
\url{https://en.wikipedia.org/wiki/Monty_Python_and_the_Holy_Grail},
or
\url{https://www.nytimes.com/2019/03/27/technology/turing-award-ai.html}. This
is for users who already know the content they want. In the backend,
the Python program is going to use the \texttt{wget}[4]
utility. Specifically, \texttt{wget -E -H -k -K -p -P path url
  robots=off} where \texttt{path} is the output directory and
\texttt{url} is the url that has been requested. To break it down:
\begin{itemize}
  \item \texttt{-E} tells \texttt{wget} to change the file extention
    if the url isn't a .html file. This allows for the downloading of
    PDF files as well as HTML files
  \item \texttt{-H} Tells \texttt{wget} that it is okay to download
    material from hosts that aren't from the specified URL. While this
    seems backwards at first, many websites host their fonts or
    pictures in a place that isn't the same as the document that is
    being request. This allows the page to appear just as it would
    when visited in a web browser
  \item \texttt{-k} This stands for ``convert links'', which means
    that when the download is complete, it converts the links on the
    page so they are sudible for browsing on the local machine. For
    example, if a blog has \texttt{otherwebsite.com/picture} on it, it
    will replace that with just \texttt{picture} to ensure that the
    browser will use the local versions of that picture
  \item \texttt{-K} This means that \texttt{wget} will make a backup
    of the HTML file when converting links with the \texttt{-k}
    option.
  \item \texttt{-p} is the most important option, as it tells
    \texttt{wget} to download all the requirements as well as the
    url. So if the site links to an outside source (such as
    \texttt{otherwebsite.com/picture}) also gets downloaded if it is
    linked in the requested url.
  \end{itemize}
All of those options ensures that downloading the URL requested get's
the website exactly as it appears in a browser, including linked
images. Additionally, it works with PDF and ZIP files, which is really
important to ensure all possible media can be obtained. This method is
also used by other parts of the program.

\subsection{Search}

Sometimes the user will not know exactly what they want, so we added
an option to get the first page of Google results (top 10
results). The \texttt{googlesearch}[5] library was very helpful for
this. This library provides a list of URLs, and then we use the URL
method to download those results (or the youtube download option if it
is a youtube link). The results are each in thier own folder nammed
based on the google search rank (1 is first result, 2 is second
result, etc). 

\subsection{YouTube}

It is well known that lot of quality educational and entiertainment
content is in video format, and the majority of that content is on
YouTube. That is why we are adding functionallity to request YouTube
videos (through a link, or a result from the search function). In this
case, the \texttt{youtube-dl} program allows for content retrevial.

\subsection{IPFS}
IPFS stands for ``InterPlanetary File System'', which is a ``A
peer-to-peer hypermedia protocol to make the web faster, safer, and
more open''[7]. The internet that is familiar to most people is the
client-server model[8], but IPFS changes that so everyone is both a
client and a server. Media is distribtued based of their crytographic
hash, a unique ID for each object instead of a URL. Anybody can add
objects, and when requesting an object, it can be downloaded from any
number of servers, not just the origional person hosting the
server. The \texttt{ipfs} command line utility[9] is used to retreive
objects.

\section{Interfaces with Other Subsystems}

Software is all about abstraction, so there are a handful of points in
which the Command and Control subsystem will interface with the other
subsystems. Ideally, all the other subsystems will work indepenedently
and a couple of links will get everything working together.

\subsection{Input - List of Requests}

Python only stores objects in memory while the program is
running. That means when the program is shut down (or the machine is
powered off), the data genereated must be saved somewhere on the
device or else that information would be lost. In this case, what is
important is to be able to store the \texttt{request} objects. This is
completed by the data managment subsystem. This means my subsystem
will call \texttt{get\_all\_requests()} which will retreive the data
about the requests, create the objects again (as they where destroyed
from memory when the program shut down), and return that information
to Command and Control.

\subsection{Output - Download path and status}

Once my subsystem completes it's donwload, it needs to update the
database about the new status. This is again through the Data
Managment subsystem. To ensure we only download each object once, each
\texttt{request} has two properties: \texttt{file\_location} and
\texttt{downloaded\_status}. Once Command and Control has completed a
download, it will call the Data Managment method
\texttt{update\_request(r)} where \texttt{r} include the changes to
\texttt{file\_location} and \texttt{downloaded\_status}. So the
Data Managmnet subsystem knows which database entry to update, we use
a Universily Uniquie Identifier (UUID)[10] to identify each
request.

\begin{figure}[h]
  \centering
  \includegraphics[scale=.5]{input-output.png}
  \caption{\begingroup \fontsize{10pt}{12pt}\selectfont Diagram
    describing the inputs and outputs of the Command and Control sub-system
    \texttt{request}. Jake Vossen, 2019-04-01 \endgroup}
\end{figure}

\end{document}

%[1] https://docs.python.org/3.7/library/multiprocessing.html
%[2] https://www.pcbenchmarks.net/number-of-cpu-cores.html
%[3] https://www.raspberrypi.org/products/raspberry-pi-3-model-b/
%[4] https://www.gnu.org/software/wget/manual/wget.html
%[5] https://github.com/MarioVilas/googlesearch
%[6] https://github.com/ytdl-org/youtube-dl/
%[7] https://ipfs.io/
%[8] https://web.cs.wpi.edu/~cs513/s07/week1-unixsock.pdf
%[9] https://docs.ipfs.io/introduction/install/
%[10] https://tools.ietf.org/html/rfc4122.html